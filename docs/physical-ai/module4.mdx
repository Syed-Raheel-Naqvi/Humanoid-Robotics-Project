# Module 4: Vision-Language-Action (VLA)

## The Convergence of LLMs and Robotics

Vision-Language-Action (VLA) represents a paradigm shift in robotics where robots can understand natural language commands, perceive their environment visually, and execute complex actions. This convergence of computer vision, natural language processing, and robotics control enables a new generation of intuitive human-robot interaction.

### Voice-to-Action: Using OpenAI Whisper for Voice Commands

The ability for robots to understand spoken commands opens up new possibilities for natural human-robot interaction. OpenAI's Whisper model provides state-of-the-art automatic speech recognition (ASR) capabilities that can be integrated into robotic systems.

Here's an example of integrating Whisper with a robotic system using ROS 2:

```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from sensor_msgs.msg import AudioData
import whisper
import torch
import pyaudio
import wave
import tempfile
import os

class VoiceToActionNode(Node):
    def __init__(self):
        super().__init__('voice_to_action_node')
        
        # Initialize Whisper model
        self.get_logger().info('Loading Whisper model...')
        self.whisper_model = whisper.load_model("base")  # Use "base" for edge devices
        self.get_logger().info('Whisper model loaded successfully')
        
        # Publisher for recognized text
        self.text_publisher = self.create_publisher(String, 'recognized_text', 10)
        
        # Publisher for robot commands
        self.command_publisher = self.create_publisher(String, 'robot_commands', 10)
        
        # Initialize audio stream
        self.audio = pyaudio.PyAudio()
        self.stream = self.audio.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=16000,
            input=True,
            frames_per_buffer=1024
        )
        
        # Timer to periodically check for audio
        self.timer = self.create_timer(0.1, self.check_audio)
        
        # Temporary file to store audio for processing
        self.temp_audio_file = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')
        self.temp_audio_file.close()
        
    def check_audio(self):
        # Read audio data from stream
        frames = []
        for _ in range(10):  # Record ~1 second of audio
            data = self.stream.read(1024)
            frames.append(data)
        
        # Save to temporary file
        wf = wave.open(self.temp_audio_file.name, 'wb')
        wf.setnchannels(1)
        wf.setsampwidth(self.audio.get_sample_size(pyaudio.paInt16))
        wf.setframerate(16000)
        wf.writeframes(b''.join(frames))
        wf.close()
        
        # Transcribe audio using Whisper
        result = self.whisper_model.transcribe(self.temp_audio_file.name)
        recognized_text = result["text"].strip()
        
        if recognized_text:
            self.get_logger().info(f'Recognized: {recognized_text}')
            
            # Publish recognized text
            text_msg = String()
            text_msg.data = recognized_text
            self.text_publisher.publish(text_msg)
            
            # Process text to generate robot commands
            robot_command = self.text_to_command(recognized_text)
            if robot_command:
                cmd_msg = String()
                cmd_msg.data = robot_command
                self.command_publisher.publish(cmd_msg)
    
    def text_to_command(self, text):
        # Simple rule-based text to command conversion
        text_lower = text.lower()
        
        if "move forward" in text_lower or "go forward" in text_lower:
            return "move_forward"
        elif "move backward" in text_lower or "go back" in text_lower:
            return "move_backward"
        elif "turn left" in text_lower:
            return "turn_left"
        elif "turn right" in text_lower:
            return "turn_right"
        elif "stop" in text_lower:
            return "stop"
        elif "pick up" in text_lower or "grasp" in text_lower:
            return "grasp_object"
        elif "wave" in text_lower:
            return "wave_arm"
        else:
            # For more complex commands, use LLM-based parsing
            return self.llm_parse_command(text)
    
    def llm_parse_command(self, text):
        # Placeholder for LLM-based command parsing
        # This would typically call an LLM API to interpret complex commands
        # such as "Go to the kitchen and bring me a red apple"
        return None

def main(args=None):
    rclpy.init(args=args)
    node = VoiceToActionNode()
    
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.stream.stop_stream()
        node.stream.close()
        node.audio.terminate()
        os.unlink(node.temp_audio_file.name)
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

### Cognitive Planning: Using LLMs to Translate Natural Language into ROS 2 Actions

Large Language Models (LLMs) can serve as high-level cognitive planners that translate human commands in natural language into sequences of executable robot actions. This involves understanding the semantics of the command, reasoning about the environment, and generating a plan.

Here's an implementation of cognitive planning using an LLM:

```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from geometry_msgs.msg import Pose
import openai
import json

class CognitivePlannerNode(Node):
    def __init__(self):
        super().__init__('cognitive_planner_node')
        
        # Subscription for high-level commands
        self.command_sub = self.create_subscription(
            String,
            'high_level_commands',
            self.command_callback,
            10
        )
        
        # Publisher for action sequences
        self.action_pub = self.create_publisher(String, 'action_sequence', 10)
        
        # Publisher for individual robot commands
        self.cmd_pub = self.create_publisher(String, 'robot_commands', 10)
        
        # Initialize OpenAI client (use your API key)
        # openai.api_key = "YOUR_API_KEY"
        
        # Current robot state (simplified)
        self.robot_location = "start"
        self.is_holding_object = False
        
        # Environment map (simplified)
        self.locations = {
            "start": {"connected": ["kitchen", "living_room"], "objects": []},
            "kitchen": {"connected": ["start", "dining_room"], "objects": ["apple", "banana"]},
            "living_room": {"connected": ["start"], "objects": ["book", "remote"]},
            "dining_room": {"connected": ["kitchen"], "objects": ["orange"]}
        }
    
    def command_callback(self, msg):
        command = msg.data
        self.get_logger().info(f"Received command: {command}")
        
        # Generate plan using LLM
        plan = self.generate_plan_with_llm(command)
        
        # Execute plan
        self.execute_plan(plan)
    
    def generate_plan_with_llm(self, command):
        # Define the system message to guide the LLM
        system_message = f"""
        You are a cognitive planning assistant for a robot. Your task is to convert natural language commands into a sequence of actions.
        
        Current robot state:
        - Location: {self.robot_location}
        - Holding: {self.is_holding_object}
        
        Environment:
        {json.dumps(self.locations, indent=2)}
        
        Possible actions:
        - Go to [location]
        - Find [object]
        - Pick up [object]
        - Drop object
        - Wave arms
        - Wait for 5 seconds
        
        Output format: JSON array of actions, like ["Go to kitchen", "Find apple", "Pick up apple"]
        Be specific and break down complex commands into simple steps.
        """
        
        try:
            # Call the LLM to generate a plan
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",  # Or gpt-4 for better results
                messages=[
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": command}
                ],
                temperature=0.1,
                max_tokens=200
            )
            
            # Extract and parse the plan
            plan_text = response.choices[0].message.content.strip()
            
            # Find the JSON part of the response
            start_idx = plan_text.find('[')
            end_idx = plan_text.rfind(']') + 1
            
            if start_idx != -1 and end_idx != 0:
                plan_json = plan_text[start_idx:end_idx]
                plan = json.loads(plan_json)
                self.get_logger().info(f"Generated plan: {plan}")
                return plan
            else:
                self.get_logger().error(f"Could not parse plan from response: {plan_text}")
                return ["error"]
        except Exception as e:
            self.get_logger().error(f"Error generating plan: {e}")
            return ["error"]
    
    def execute_plan(self, plan):
        if plan == ["error"]:
            self.get_logger().error("Plan generation failed")
            return
            
        # Publish the action sequence
        plan_msg = String()
        plan_msg.data = json.dumps(plan)
        self.action_pub.publish(plan_msg)
        
        # Execute each action in sequence
        for action in plan:
            self.get_logger().info(f"Executing action: {action}")
            
            # Convert action to ROS command
            cmd = self.action_to_ros_command(action)
            if cmd:
                cmd_msg = String()
                cmd_msg.data = cmd
                self.cmd_pub.publish(cmd_msg)
                
                # Simple delay to allow action to complete
                # In a real system, you'd use action servers with feedback
                self.get_clock().sleep_for(rclpy.duration.Duration(seconds=2))
    
    def action_to_ros_command(self, action):
        action_lower = action.lower()
        
        if "go to" in action_lower:
            location = action_lower.replace("go to", "").strip()
            return f"navigate_to_{location}"
        elif "find" in action_lower:
            obj = action_lower.replace("find", "").strip()
            return f"find_object_{obj}"
        elif "pick up" in action_lower or "pick up" in action_lower:
            obj = action_lower.replace("pick up", "").replace("grasp", "").strip()
            return f"grasp_object_{obj}"
        elif "drop" in action_lower:
            return "release_object"
        elif "wave" in action_lower:
            return "wave_arms"
        elif "wait" in action_lower:
            return "idle"
        else:
            return "idle"
```

### Capstone Project: The Autonomous Humanoid

The capstone project brings together all concepts from the course in a comprehensive autonomous humanoid system. The robot receives a voice command, processes it using VLA, plans a path, navigates obstacles, identifies an object using computer vision, and manipulates it.

#### System Architecture:

1. **Voice Input**: Whisper transcribes spoken commands
2. **Cognitive Planning**: LLM interprets command and generates action plan
3. **Perception**: Computer vision identifies objects and environment
4. **Navigation**: Path planning and obstacle avoidance
5. **Manipulation**: Object grasping and manipulation
6. **Control**: Low-level motor control for humanoid movements

Here's a simplified implementation of the capstone system:

```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from geometry_msgs.msg import Twist, Pose
from sensor_msgs.msg import Image, LaserScan
from cv_bridge import CvBridge
import cv2
import whisper
import numpy as np

class CapstoneHumanoidNode(Node):
    def __init__(self):
        super().__init__('capstone_humanoid_node')
        
        # Initialize Whisper model for voice commands
        self.whisper_model = whisper.load_model("base")
        
        # Initialize CV bridge
        self.cv_bridge = CvBridge()
        
        # State variables
        self.state = "waiting_for_command"  # waiting_for_command, executing_plan, etc.
        self.current_plan = []
        self.current_step = 0
        
        # Subscriptions
        self.voice_cmd_sub = self.create_subscription(
            String, 'voice_commands', self.voice_command_callback, 10
        )
        self.image_sub = self.create_subscription(
            Image, 'camera/image_raw', self.image_callback, 10
        )
        self.laser_sub = self.create_subscription(
            LaserScan, 'scan', self.laser_callback, 10
        )
        
        # Publishers
        self.cmd_vel_pub = self.create_publisher(Twist, 'cmd_vel', 10)
        self.status_pub = self.create_publisher(String, 'system_status', 10)
        
        # Timer for state machine
        self.timer = self.create_timer(0.1, self.state_machine)
        
    def voice_command_callback(self, msg):
        if self.state == "waiting_for_command":
            command = msg.data
            self.get_logger().info(f"Received voice command: {command}")
            
            # Process command to generate plan
            self.current_plan = self.process_voice_command(command)
            self.current_step = 0
            self.state = "executing_plan"
    
    def process_voice_command(self, command):
        # Simplified command processing
        # In a real system, this would involve LLM-based planning
        if "go to kitchen" in command.lower():
            return ["navigate_to_kitchen"]
        elif "find red ball" in command.lower():
            return ["navigate_to_living_room", "find_red_ball"]
        else:
            return ["idle"]
    
    def image_callback(self, msg):
        # Process camera data for object recognition
        cv_image = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")
        
        # Simple color-based object detection (for demonstration)
        # In a real system, this would use deep learning models
        hsv = cv2.cvtColor(cv_image, cv2.COLOR_BGR2HSV)
        
        # Detect red objects
        lower_red = np.array([0, 50, 50])
        upper_red = np.array([10, 255, 255])
        mask1 = cv2.inRange(hsv, lower_red, upper_red)
        
        # Upper red range
        lower_red = np.array([170, 50, 50])
        upper_red = np.array([180, 255, 255])
        mask2 = cv2.inRange(hsv, lower_red, upper_red)
        
        # Combine masks
        mask = mask1 + mask2
        
        # Find contours
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if contours:
            # Find largest contour
            largest_contour = max(contours, key=cv2.contourArea)
            if cv2.contourArea(largest_contour) > 500:  # Minimum area threshold
                # Object detected
                self.handle_object_detection()
    
    def laser_callback(self, msg):
        # Process laser data for obstacle detection
        # Check for obstacles in front of robot
        front_scan = msg.ranges[len(msg.ranges)//2 - 10 : len(msg.ranges)//2 + 10]
        
        if min(front_scan) < 0.5:  # Obstacle within 0.5m
            self.handle_obstacle_detection()
    
    def handle_object_detection(self):
        if self.state == "executing_plan" and "find_red_ball" in self.current_plan:
            self.get_logger().info("Red ball detected!")
            # Stop and attempt to grasp
            self.stop_robot()
            self.attempt_grasp()
    
    def handle_obstacle_detection(self):
        if self.state == "executing_plan":
            self.get_logger().info("Obstacle detected, planning alternative route")
            # Implement obstacle avoidance
            self.execute_obstacle_avoidance()
    
    def state_machine(self):
        if self.state == "executing_plan" and self.current_step < len(self.current_plan):
            current_action = self.current_plan[self.current_step]
            
            if current_action == "navigate_to_kitchen":
                self.navigate_to("kitchen")
            elif current_action == "find_red_ball":
                self.search_for_object("red_ball")
            
            # In a real system, you'd check completion status before advancing
            # For this example, we'll advance after a delay
            self.current_step += 1
        elif self.current_step >= len(self.current_plan):
            self.get_logger().info("Plan completed")
            self.state = "waiting_for_command"
            self.publish_status("Plan completed successfully")
    
    def navigate_to(self, location):
        # Simplified navigation
        cmd = Twist()
        cmd.linear.x = 0.2  # Move forward
        self.cmd_vel_pub.publish(cmd)
        self.publish_status(f"Navigating to {location}")
    
    def search_for_object(self, obj_name):
        # Simplified object search
        cmd = Twist()
        cmd.angular.z = 0.5  # Rotate to search
        self.cmd_vel_pub.publish(cmd)
        self.publish_status(f"Searching for {obj_name}")
    
    def stop_robot(self):
        cmd = Twist()
        self.cmd_vel_pub.publish(cmd)  # Zero velocity stops robot
    
    def attempt_grasp(self):
        # Placeholder for grasping action
        self.publish_status("Attempting to grasp object")
    
    def execute_obstacle_avoidance(self):
        # Simplified obstacle avoidance
        cmd = Twist()
        cmd.angular.z = 0.5  # Turn to avoid obstacle
        self.cmd_vel_pub.publish(cmd)
    
    def publish_status(self, status):
        msg = String()
        msg.data = status
        self.status_pub.publish(msg)

def main(args=None):
    rclpy.init(args=args)
    node = CapstoneHumanoidNode()
    
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

### Weekly Breakdown: Week 13 - Conversational Robotics

#### Week 13: Integrating GPT Models for Conversational AI in Robots
- Integrating LLMs with ROS 2 for natural language understanding
- Speech recognition and natural language processing
- Multi-modal interaction: combining speech, gesture, and vision
- Final capstone project implementation and testing

### Practical Exercise: The Autonomous Humanoid Capstone

For the capstone project, you'll implement a complete system that:

1. Receives a voice command using OpenAI Whisper
2. Uses an LLM to interpret the command and generate an action plan
3. Navigates to the required location while avoiding obstacles
4. Identifies and localizes a specific object using computer vision
5. Manipulates the object with the robot's end effectors

The project should demonstrate integration of all four modules:
- ROS 2 for system integration and communication
- Gazebo/Isaac Sim for testing and development
- NVIDIA Isaac for perception and navigation
- VLA for natural language interaction

Your system should handle at least three different types of commands:
- Navigation commands ("Go to the kitchen")
- Object interaction commands ("Find the red apple")
- Complex multi-step commands ("Go to the living room and pick up the blue ball")

### Assessments for Module 4

- **Voice Command Processing**: Implement Whisper-based speech recognition with 90%+ accuracy on clear commands
- **LLM-Based Planning**: Demonstrate LLM-based interpretation of natural language into action sequences
- **Multi-Modal Integration**: Successfully combine speech, vision, and action in a coherent system
- **Capstone Project**: Implement the complete Autonomous Humanoid system that responds to voice commands, navigates, and manipulates objects

### Hardware Requirements for VLA Systems

The Vision-Language-Action pipeline has specific hardware requirements for real-time performance:

1. **Edge Processing**:
   - **NVIDIA Jetson Orin Nano (8GB)**: For running Isaac ROS packages and computer vision
   - **Intel RealSense D435i**: For RGB-D sensing (essential for 3D perception)
   - **USB Microphone Array (ReSpeaker)**: For far-field voice command recognition

2. **Cloud Processing** (optional for heavy LLMs):
   - Access to OpenAI API or similar service for cognitive planning
   - High-speed internet connection for low-latency cloud interaction

3. **Simulation Requirements**:
   - PC with RTX 4070 Ti or higher for running Isaac Sim
   - Ubuntu 22.04 LTS with ROS 2 Humble Hawksbill

### The Future of VLA in Robotics

The Vision-Language-Action paradigm represents the future of intuitive human-robot interaction. As these systems become more sophisticated, we can expect robots that:

- Understand complex natural language commands
- Perceive and reason about their environment at human-like levels
- Execute complex tasks requiring both dexterity and cognitive reasoning
- Learn from natural human instruction and demonstration

This convergence of technologies is enabling robots to move from rigid, pre-programmed systems to adaptive, intelligent agents capable of complex interactions in our human-centered world.

---

## Thought-Provoking Questions

1. How might natural language interfaces change the way humans interact with robots in everyday environments?
2. What are the challenges of real-time processing in Vision-Language-Action systems?
3. How could VLA systems enable robots to learn new tasks through natural human instruction?